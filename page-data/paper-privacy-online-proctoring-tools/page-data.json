{"componentChunkName":"component---src-templates-post-js","path":"/paper-privacy-online-proctoring-tools/","result":{"data":{"site":{"siteMetadata":{"title":"Andrea Franchini"}},"markdownRemark":{"id":"3750674f-0026-5462-a556-5841bcd6cdcb","excerpt":"Abstract This paper analyzes issues found in online proctoring tools (OPT),\naddressing in particular privacy and fairness concerns about this kind\nof software…","html":"<h3>Abstract</h3>\n<p>This paper analyzes issues found in online proctoring tools (OPT),\naddressing in particular privacy and fairness concerns about this kind\nof software. While OPT are a solution that allows institutions to hold\nexams during pandemics, the way they work represents an invasion of\nstudents’ privacy, raising the question of whether said invasion is\njustified in the name of academic integrity.</p>\n<p>The lack of external regulation about OPT use of artificial intelligence\n(AI) results sometimes in biased systems that can discriminate students.</p>\n<p>OPT that enforce strict requirements may also put at disadvantage\ncertain individuals.</p>\n<h2>Introduction</h2>\n<p>Online proctoring tools (OPT) are a category of software that allow\nstudents to take exams remotely under strict supervision provided by a\nsoftware, a human, or both.</p>\n<p>While OPT have been around for more than a decade, the sudden shift to\nremote learning caused by the COVID-19 pandemic produced a surge in\nadoptions of such kind of software.</p>\n<p>OPT allow institutions to hold exams sessions while preventing students\nfrom cheating (e.g.: using hidden notes, communicating with others or\neven having another person taking the exam in their place).</p>\n<p>Because of their sudden and widespread adoption, there has been little\ntime for institutions, and people in general, to address concerns about\nprivacy and fairness offered by these tools, especially when deployed at\nsuch massive scale.</p>\n<p>I believe that there is need for stricter regulations about OPT, as\nprivacy and fairness play have a key role in this kind of software.</p>\n<p>This topic is crucial since we do not know how long this software will\nbe adopted by institutions and the side effects they might have in the\nnear-future. Major control over this technology can help avoid\nunpleasant situations for students.</p>\n<p>With “privacy” I mean the fundamental right of students to\nhave the bounds of their personal space respected by others. In a normal\nscenario, an institution should not have access or be interested in\ndetails of a student’s personal life, beside the personal\ndata needed for bureaucratic and administrative reasons.</p>\n<p>Whenever institutions overstep this initially set bound of their\nrelations with students, we shall evaluate if it is necessary, and why.</p>\n<p>In an education environment, “fairness” goes two ways.</p>\n<p>The first is the students’ relationship towards their\ninstructors, meaning that students have to pledge to the institution\ncode of conduct (e.g.: not to cheat during exams or engage in other\nforms of dishonesty). We may call this “academic\nintegrity”.</p>\n<p>The other is an institution relationship towards its students, in which\nthe first must guarantee inclusiveness, prevent discriminations of any\nkind from happening and ensure impartiality in evaluations of\nstudents’ work. For example, exams should be accessible to\nall student regardless of ethnicity and gender, and account for students\nwith disabilities.</p>\n<p>Of course, privacy and fairness are related: when respecting the privacy\nof students, instructors may be more impartial during grading because\nthey lack knowledge of private details that might make them biased\ntowards certain students.</p>\n<p>Better understanding of OPT can help both students and institutions in\nhighlighting and addressing typical concerns of this technology; in\nparticular, students should be better informed of how OPT work, and how\nthese tools may represent a risk for their privacy and fair-treatment.</p>\n<p>In this paper, I will try to offer compelling arguments on how\nstudents’ right to privacy is often violated and how\nfairness is not always ensured when employing OPT.</p>\n<p>How OPT and Remote Exams work</p>\n<p>Before introducing my arguments, I want to better explain what an OPT\nconsists of and how a remote exam works.</p>\n<p>An OPT is composed roughly of two macro-components: the student’s one\nand the institution’s.</p>\n<p>The first consists of a software run by the student’s\ncomputer under the form of a web browser extension or application for\nthe operative system. This software accesses the webcam and microphone\nconnected to the student’s computer, in order to record\ntheir behaviors during exams; it may also require students to share what\nthey see on the screen with their instructors.</p>\n<p>In certain cases, it can also prevent the student from using other\nsoftwares unless explicitly allowed.</p>\n<p>The latter is used to process and store said recording of students,\nshowing them to instructors during or after an exam, and flag suspect\nbehaviors of students.</p>\n<p>The most well-known and diffused OPT are commercial proprietary software\nthat is licensed to institutions. Most OPT companies also provide\nhosting solutions for institutions, meaning that recordings are stored\non the company servers instead of the institution servers.</p>\n<p>A remote exam requires students to authenticate themselves in order to\nprove their identity, usually by framing a valid document, and to scan\ntheir room, to show that nobody else is present and that they do not\nhave any unapproved material.</p>\n<p>Once the exam starts, students need to remain framed by the webcam for\nits whole duration, and suspect behaviors such staring in another\ndirection or people talking are notified to both the student and the\nproctor (human or virtual).</p>\n<p>This analysis is usually automated and carried out by an artificial\nintelligence (AI).</p>\n<p>Students also must not interact with any other computer application if\nthe latter is not already blocked by the proctoring software.</p>\n<p>Depending on the configuration, an OPT may require a review by a human\nproctor before disqualifying a student when enough reports of suspect\nbehavior are present.</p>\n<h2>Privacy</h2>\n<p>The first privacy issue with OPT is that such tools require students to\nrecord not only themselves, but also their entire room. This causes OPT\nto collect a huge quantity of data that are related to the student, but\nultimately unrelated to the exam. Some services such as ProctorExam even\nrequire the use of a smartphone as an additional point of view to record\nthe back of the student, therefore obtaining a detailed, 360-degrees\nview of the room. In addition, there is a chance to expose personal\ndetails of a student private life when they are required to scan their\nrooms in the authentication process. Some students may agree with such\nmethods, but others may not want to be forced to expose personal spaces\nto an external eye, especially when they must share a room with others\nduring lockdowns caused by a pandemic. Because the perception of what is\nprivate is often subjective, it is difficult to make absolute\nconclusions about it.</p>\n<p>In my opinion a full scan of the room is an invasion of the\nstudents’ privacy from an ethical point of view for the\nfollowing reasons.</p>\n<p>In a traditional exam held in a classroom or similar facility, students\ncan see who is watching over them, and beside their physical person, no\nother personal information is directly deducible. In a remote exam\ncontext, though, the overseer is no longer in students’\nreach: it becomes an abstract entity that do not directly interact with\nthem; it may also not be their instructor, but an employee of an OPT\ncompany.</p>\n<p>Let’s make the hypothesis that in an exam held in the\nclassroom the privacy of both students and their proctors is respected:\nboth sides have few ways to violate each other privacy, and if they did,\nit would be immediately witnessed by others.</p>\n<p>In a remote setting students cannot know who, if any, is watching their\nrecording, and what are they looking for. Often, students do not even\nhave access to their own recordings, because they are managed by the\ninstitution on their behalf.</p>\n<p>Thus, the access to private information on each side is no longer\nsymmetric, and students have more to lose in a remote exam than in a\nclassroom exam. Students need to sacrifice their right to privacy to be\nable to take exams: if no alternatives are offered, though, it is very\nlikely that students would rather take an exam with a tool they do not\nagree on, rather than forfeiting it, putting them in a difficult\nposition.</p>\n<p>The idea of having a stranger peeking in one’s room is\noften unsettling for some people, and students are even more legitimated\nto be wary of such tools when many universities decided to have\nmandatory proctored exams by using OPT. Of course, this led to\nwidespread protests from students (Kelley 2020).</p>\n<p>A possible counter-argument by supporters of OPT is that since students\nalready have to allow an invasion of their privacy in the classroom, by\nbeing closely monitored by instructors, remote surveillance is ethically\nequivalent in-classroom surveillance. An excellent confutation is\nprovided by (Coghlan, Miller, and Paterson 2020), who state that to make\nin-classroom exams truly equivalent to remote ones, there should be a\nproctor in front of each student staring at them for the whole duration\nof their exam, possibly with an AI assistant to analyze each minuscule\nface movement.</p>\n<p>I find this last argument plausible because it shows that remote\nsurveillance, thanks to the presence of AI tools, allows instructors to\nmonitor students in greater detail, and that such strict surveillance if\nactually carried out in-class would be definitely unsettling to a\nstudent. In addition, OPT allow a single instructor to monitor many\nstudent at greater detail, a feat that would be unrealistic in a\ntraditional classroom, further proving the non-equivalence between\nremote and in-class proctoring.</p>\n<p>I believe that OPT and institutions should address with more\nconsideration students that desire a higher level of privacy. Let us\nconsider the following example: student A wants to share the least\namount of private details with the proctor, while student B does not\ncare. If we regard mostly B’s view, possibly requiring them\nto share a detailed recording of their room, then A has more to lose\nthan B, namely B’s privacy. If we consider the opposite\ncase, though, since B does not care, A is in a better position. Thus, in\nthis kind of situations, it is a better compromise to second\nB’s needs.</p>\n<p>We might also want to examine a possible reason for B lack of interest:\nperhaps B was not informed in a direct way of what are the implication\nof the use of OPT. While OPT clearly explain to students which of their\ndata they will have access to, but how these data will be processed in\ndetail is not specified.</p>\n<p>While this example seems to work, we are not considering an important\nfactor, which is the role of the proctor (human or virtual). To prevent\ncheating, institutions employ OPT to make sure that cheating material\nand other people are not present.</p>\n<p>First, the system receives the integral audio and video recording of\nstudents and stores them, then analyzes them to determine whether there\nwas an instance of cheating.</p>\n<p>It is obvious that OPT end up with lots of irrelevant data for their\npurpose, such as a picture a student may have on their wall, or how big\nis their house. Then why and for how long these private data should be\nstored when they no longer serve a purpose?</p>\n<p>An immediate answer would probably state to discard them immediately, as\nthey no longer serve a purpose.</p>\n<p>There is actually little reason for a proctoring tool company to\nactively discard such data that is embedded in a video recording, as it\nwould mean to actively dedicate resources to edit out the details, for\nexample by blurring them.</p>\n<p>To a machine trained to recognize objects like smartphones, since it is\nnot a sentient being, it should not really matter who the subject of a\nphoto hanging from the wall is (except in cases where bias is involved\nwhich I will analyze in the next section). It also should not affect the\nsecurity of students unless a malicious design is involved.</p>\n<p>With a human proctor, the situation is a different: there is no\ntechnologic protection put in place to make sure that a proctor does not\nfocus too much on details that are already deemed unnecessary. Most\nimportantly, they must not disclose or use for other purposes those\nrecordings without consent, for example, by taking a picture or a video\nwith their smartphones and sharing it online. Should that happen, that\nwould result in a massive violation of a student’s privacy.</p>\n<p>In addition, companies often reserve their right to retain recording for\nlong period of time, for a maximum of five years in the case of\nRespondus and two years in the case of ProctorU, or they delegate this\nright to institutions, as ProctorExam does: in both cases, it is out of\nstudents’ control whenever this data is deleted or\naccessed.</p>\n<p>A question that we must ask ourselves is whether such invasion of\nprivacy is justified in name of the academic integrity.</p>\n<p>While the interest of institution in preventing cheating is clearly\nlegitimate from an academic point of view, and it is already enforced\nduring in-class exams, the way OPT have been quickly adopted makes me\nthink that these tools have been seen as an easy solution to the complex\nproblem that is remote teaching. I said “easy solution”\nbecause OPT have been developed with one and only one purpose, that is\nto detect students that cheat. Other “more complex”\nsolutions, such as rethinking the structure of an exam to make cheating\nharder, clearly require effort from the instructors and therefore this\noption usually is not immediately considered as an alternative.</p>\n<h2>Fairness</h2>\n<p>One critical issue is the perceived fairness of OPT. My argument is that\ndue to their unclear inner workings, it’s hard to prove that OPT are\ntruly fair and unbiased in their job and use.</p>\n<p>The first immediate consideration about OPT is how little we know about\nhow they function internally.</p>\n<p>I will argue that it is not possible to prove the fairness of OPT if we\ncannot trust them first.</p>\n<p>This is caused by two factors: the use of AI, and the proprietary\nlicense of these tools.</p>\n<p>I’ll address first the proprietary nature of this kind of softwares.\nSince OPT are commercial products, it is in their owners’ interest to\nprevent their competitors from learning how their software works,\ntherefore their source code is often kept secret. In addition, OPT\ncompanies usually offer their product as a service, meaning that the\nsoftware runs on their servers and the data is usually stored in a\ndatacenter under their control. This means that an external party has no\nway to know how an online proctoring system works: it is in fact a\nblackbox to them.</p>\n<p>In my opinion, OPT being blackboxes represent a major concern about the\ntrust we put into these systems. Although companies must respect strict\nrequirements about how they must handle their customers’ data, security\nbreaches are possible, as in the case of ProctorU (Abrams 2020), and\nrepresent a threat to the students’ privacy and the trust that student\nare required to put in these companies.</p>\n<p>I believe that if these companies open-sourced at least parts of their\ncode, there would be a great benefit for all: third-party analysis may\nhighlight flaws that a company didn’t notice, concerned users might\nhave the possibility to better understand how the software works and\ncompanies would still be able to monetize their product through licenses\nand by providing the service just as they already do.</p>\n<p>I think that (Hoepman and Jacobs 2007) offer a detailed analysis on why\nopen source is not a liability for companies that handle sensible data,\nshowing that it’s more convenient to trust a company that opens its\nsoftware and design to a public inspection than one that keep the inner\nworkings secret.</p>\n<p>This could help build trust between students and proctoring tools, and,\nmost importantly, allow institutions to make a more informed choice\nabout which OPT to adopt.</p>\n<p>The other reason why it is hard to understand how an OPT works is the\nuse of AI.</p>\n<p>With the advent of machine learning, most AI are trained on huge\ndatasets, which makes them harder to replicate and to understand.</p>\n<p>Since AI are great at repeating complex task in a shorter time than\nhumans, they are used in OPT for two purposed: speed up authentication,\nand identify suspect behaviors.</p>\n<p>On paper, AI should do their job without issues but in reality, they\nhave many edge cases in which they fail. Yet, because of their complex\nnature, it is hard for a company to pinpoint the issue, and even harder\nfor an external viewer to figure out why the tool is not manifesting the\ncorrect behavior.</p>\n<p>When an AI fails to do its job, or does it wrong, there are serious\nimplications. It is also important to understand what is behind a\nmisbehaving AI.</p>\n<p>One common issue with AI used to analyze images of humans is that they\noften fail to recognize certain individuals.</p>\n<p>This is often the result, in the best scenario, of an oversight from the\nAI designers and in the worst, of an intentional malicious design: in\nboth cases, the result is a biased AI, that often produce racist or\nsexist outputs. For example, people of color have trouble being\nrecognized by such systems, that either think that no person is present,\nor that there’s a lack of light. Another example is how a rigid\nclassification into ”male” or ”female” that simplify the development\nof an AI fails to properly categorize transgender and non-binary people\n(Gebru 2019).</p>\n<p>In a OPT authentication process, a student ID is compared to what the\nwebcam is recording, usually to prevent someone else to impersonate a\nstudent. If a webcam isn’t able to detect a face, or the algorithm\nthinks that the person visible to the webcam does not match the student\nID details, it may deny them the possibility of taking an exam, or\nrequire the intervention of a human proctor.</p>\n<p>My belief is that such a strict system is unfair toward students for the\nfollowing reason. The only advantage of an automated process is a faster\nadmission to an exam for many, possibly white, gender-conforming\nstudents, at the expenses of a minorities that need assistance because\nof faulty software. This is not an acceptable compromise for any\ninstitution that values the equal-treatment of their students, because\nif the software consistently misbehaves in presence of certain\nindividuals, it clearly does not ensures that the latter are treated\nfairly.</p>\n<p>Another drawback is that students forced to require assistance may also\ngo through embarrassing and possibly traumatic moments: for example, a\ntransgender student may be outed by the system to a stranger, and they\nneed to be notified of the possibility of such situations (Swauger\n2020).</p>\n<p>These cons lead me to think that AI-based authentication systems are\nlargely unnecessary because they try to make the manual authentication\nof students a problem, when it was not in first place: instructors could\nverify identity of students manually in a similar fashion to what\nhappens in a in-class exam, or, if the time is a concern, during the\nexam. It is unclear to me, though, why a company would push for the use\nof AI for authentication as a solution.</p>\n<p>There are still issues that happen during an exam. The first one is how\nOPT present a classist view in how they expect a student to setup their\nworkspace in preparation to an exam, the second one is what should an\nOPT see as a presence of misconduct.</p>\n<p>OPT, as already said, require students to setup their workspace\nadequately, meaning that the ambient should be well lit, without noise,\npeople, pets and that they should be visible, possibly from two angles.\nMy argument is that the requirement of OPT are not fair towards certain\ncategory of students.</p>\n<p>Especially during lockdowns, students who live in a one-room flat with\ntheir own family may not be able to find sufficient space to satisfy the\nOPT demands. It may be impossible for a student whose parents work from\nhome and have little siblings to find the necessary space and quiet in\norder to take an exam: it is likely that someone else will end up\nrecorded by the webcam. Beside the student, this kind of situation\ncreates uneasy conditions for others: for example, students living with\ntoddlers can not be sure that there will be silence during their exams.</p>\n<p>Since OPT require an Internet connection with a fair amount of\nbandwidth, this might hinder tasks like remote working, not be a reality\nin first place, especially for students living in remote areas.</p>\n<p>Comparing this scenario to one in which students have their own room,\nOPT favors the latter, thus treating the first unfairly.</p>\n<p>Noisy neighbors might also be a problem, as not all houses have a good\nsound insulation and load voices may be flagged by system.</p>\n<p>I believe that institutions that do not allow alternative methods or\nprovide students with a public space with all due safety precautions\nwhere to take exams are unfair in the treatment of their students, in\nthis case with proctoring tools representing as a gap between students\nwho can afford better conditions (a larger place, stable Internet\nconnections) and those who cannot.</p>\n<p>Similar concerns are raised by (Coghlan, Miller, and Paterson 2020), who\nalso propose that universities might loan devices for students who\ncannot afford a computer or webcam, as well as allowing selected student\nto take their exams on campus. They highlight how this solution might\ncause delays in courses and other logistical issues. While I agree with\nsuch solutions, I believe that offering students alternatives in these\nkind of situations should be a priority for any institute.</p>\n<p>During exams, OPT analyze student behaviors in search of possible sign\nof cheating. I believe it is quite hard to make an arbitrary list of\nwhat is cheating and what is not. Therefore, designers of OPT have tried\nto guess what is a likable sign of cheating. Turning the head, averting\nthe gaze from the computer screen, but also scratching and other minor\nmovements are sufficient to get a student behavior flagged as\nsuspicious.</p>\n<p>In my opinion watching out for such a huge number of factors is\nexcessive, especially in light of evidence that not always a directional\nchange of a student’s gaze represents an instance of cheating;\nnonetheless many behaviors that can flag a student are also signs of\nanxiety (Kolski and Weible 2019).</p>\n<p>This creates unfavorable conditions for students that have disabilities\nor more generally have involuntary behaviors caused by anxiety or tics.</p>\n<p>I believe that systems that are too strict, notify students too much or\neven stop their exams for some seconds in presence of suspicious\nbehaviors can negatively impact students’ performances an produce an\nunnecessary uncomfortable experience.</p>\n<p>Even for neurotypical, able-bodied students it is natural to stretch,\nroll up eyes in front of a difficult question, or softly re-read their\nanswers to relieve stress, yet such behaviors are supposed to be a sign\nof cheating.</p>\n<p>Being flagged repeatedly may cause a termination of a student exam, in\ncertain cases. To prevent this kind of situations, the decision of what\nto do with flagged students falls to a human proctor, when present. At\nfirst we might think that the presence of a proctor would make a system\nless biased, but it in reality people can over-trust AI or may be unsure\non how to interpret the findings presented to them by the AI (Coghlan,\nMiller, and Paterson 2020). I believe that the claim of human proctors\nnot being completely reliable is plausible, because a proctor who isn’t\nfamiliar with the student might not be aware of a student possible\nconditions. An instructor that sees a student repeatedly flagged may\nalso want to question them separately to assert their effective\nknowledge of the exam’s subject: while it is the instructor’s job, it\nmay be unfair for a student whose behavior would have otherwise gone\nunnoticed in class, thus causing unnecessary stress.</p>\n<p>I want now to discuss if it is possible to ensure fairness in OPT.</p>\n<p>I believe that in their current form OPT are more of a trouble to\nstudents than an helpful tool. They represent a vision of technology as\nbarrier to illicit behaviors such as cheating, rather than an enabler of\nnew way of teaching and assessing students’ abilities.</p>\n<p>While a poorly designed AI might be eventually fixed, I believe that its\nuse can still raise concerns because of the difficulty in understanding\nsuch tools, thus the need of stricter regulations pertaining the use of\nAI with personal data.</p>\n<p>The idea of preventing cheating is clearly motivated by the will of\ninstitutions in ensuring a fair exam to students, and I agree with such\nview, but extra care needs to be taken to ensure that the exam is fair\ntowards students with disabilities or who are neurodivergent.</p>\n<h2>Conclusions</h2>\n<p>While OPT represented an immediate and easy solution for institutions\nlooking for a way to hold exams during a pandemic, few concerns have\nbeen raised about the use of such technology, in particular about\nprivacy and fairness.</p>\n<p>These tools invade the students’ privacy and collect huge amount of\nunnecessary data, a far greater amount than what could be gathered from\nan equivalent, in-class scenario. The way recording of students are\nstored and accessed also raise concern regarding their illegitimate use.</p>\n<p>Fairness has to be ensured when the design of such tool requires the use\nof AI to process recording of students, in order to prevent\ndiscrimination and false positive caused by students that have\ndisabilities or are neurodivergent.</p>\n<p>Today, these tools are common despite the lack of strict and specific\nregulations about their use, which are needed to better safeguard\nstudents’ privacy and rights to a fair education.</p>\n<p>Institutions should take additional care in deciding whether to use\nthese tools, inform students in detail about how these tools work, and\nto opportunely configure them in order to allow student major control\nover their data.</p>\n<h2>Bibliography</h2>\n<p>Abrams, Lawrence. 2020. “ProctorU Confirms Data Breach\nafter Database Leaked Online.” BleepingComputer. August 9, 2020.\n<a href=\"https://www.bleepingcomputer.com/news/security/proctoru-confirms-data-breach-after-database-leaked-online/\">https://www.bleepingcomputer.com/news/security/proctoru-confirms-data-breach-after-database-leaked-online/</a>.</p>\n<p>Coghlan, Simon, Tim Miller, and Jeannie Paterson. 2020.\n“Good Proctor Or ”Big Brother”? AI Ethics and Online Exam\nSupervision Technologies.” ArXiv Preprint ArXiv:2011.07647.</p>\n<p>Gebru, Timnit. 2019. “Oxford Handbook on AI Ethics Book\nChapter on Race and Gender.” ArXiv Preprint ArXiv:1908.06165.</p>\n<p>Hoepman, Jaap-Henk, and Bart Jacobs. 2007. “Increased\nSecurity through Open Source.” Communications of the ACM 50 (1): 79—83.\n<a href=\"https://doi.org/10.1145/1188913.1188921\">https://doi.org/10.1145/1188913.1188921</a>.</p>\n<p>Kelley, Jason. 2020. “Students Are Pushing Back against\nProctoring Surveillance Apps.” Electronic Frontier Foundation. September\n25, 2020.\n<a href=\"https://www.eff.org/deeplinks/2020/09/students-are-pushing-back-against-proctoring-surveillance-apps\">https://www.eff.org/deeplinks/2020/09/students-are-pushing-back-against-proctoring-surveillance-apps</a>.</p>\n<p>Kolski, Tammi, and Jennifer Weible. 2018. “Examining the\nRelationship between Student Test Anxiety and Webcam Based Exam\nProctoring.” Online Journal of Distance Learning Administration 21.</p>\n<p>ProctorExam. Last accessed Feb 2021.\n<a href=\"https://proctorexam.com/\">https://proctorexam.com/</a>.</p>\n<p>ProctorU. Last accessed Feb 2021.\n<a href=\"https://www.proctoru.com/\">https://www.proctoru.com/</a>.</p>\n<p>Respondus. Last accessed Feb 2021.\n<a href=\"https://web.respondus.com\">https://web.respondus.com</a>.</p>\n<p>Swauger, Shea. 2020. “Our Bodies Encoded: Algorithmic Test\nProctoring in Higher Education.” In Critical Digital Pedagogy.\nPressbook.\n<a href=\"https://cdpcollection.pressbooks.com/chapter/our-bodies-encoded-algorithmic-test-proctoring-in-higher-education/\">https://cdpcollection.pressbooks.com/chapter/our-bodies-encoded-algorithmic-test-proctoring-in-higher-education/</a>.</p>","frontmatter":{"title":"Do online proctoring tools ensure students' privacy and fair-treatment?","date":"2021","description":"A paper I wrote for my Computer Ethics Class about remote exams and the tools used to monitor students' behavior, pondering whether this kind of software poses a threat to student privacy and equal conditions. It is not particularly long, but I woudln't say it's short either."}}},"pageContext":{"id":"3750674f-0026-5462-a556-5841bcd6cdcb","previous":"97a3ba5a-1ea6-53da-b75f-1068fd7fe16d","next":null}},"staticQueryHashes":["3000541721"]}